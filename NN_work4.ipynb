{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4711554",
   "metadata": {},
   "source": [
    "## NN work on DIS reconstruction 4\n",
    "\n",
    "Trying out things from Ben's example.\n",
    "\n",
    "Uses set of variables is E, pt, pz, Empz (redundant but useful?) for both HFS and electron as well as delta phi between HFS and electron.\n",
    "\n",
    "Has x_meas / x_true resolution plots in bins of true y.\n",
    "\n",
    "Experimenting with training hyperparameters.\n",
    "\n",
    "### Learning rate of 1e-4\n",
    "\n",
    "This gives the best results!  Faster than 1e-5 with less artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2553292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owen/.pyenv/versions/3.8.1/lib/python3.8/site-packages/pandas/compat/__init__.py:109: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import rc\n",
    "from numpy import inf\n",
    "import os\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "import uproot3\n",
    "\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f90f62",
   "metadata": {},
   "source": [
    "## Read in the minitree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750ccba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " np.pi = \n",
      "3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "print('\\n np.pi = ')\n",
    "print(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7d3ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'minitree;1']\n",
      "<TTree b'minitree' at 0x00013d5257c0>\n",
      "Q2_e                       (no streamer)              asdtype('>f4')\n",
      "Q2_sigma                   (no streamer)              asdtype('>f4')\n",
      "Q2_esigma                  (no streamer)              asdtype('>f4')\n",
      "Q2_da                      (no streamer)              asdtype('>f4')\n",
      "gen_Q2                     (no streamer)              asdtype('>f4')\n",
      "y_e                        (no streamer)              asdtype('>f4')\n",
      "y_sigma                    (no streamer)              asdtype('>f4')\n",
      "y_esigma                   (no streamer)              asdtype('>f4')\n",
      "y_da                       (no streamer)              asdtype('>f4')\n",
      "HFS_px                     (no streamer)              asdtype('>f4')\n",
      "HFS_py                     (no streamer)              asdtype('>f4')\n",
      "HFS_pz                     (no streamer)              asdtype('>f4')\n",
      "gen_y                      (no streamer)              asdtype('>f4')\n",
      "e_px                       (no streamer)              asdtype('>f4')\n",
      "e_py                       (no streamer)              asdtype('>f4')\n",
      "e_pz                       (no streamer)              asdtype('>f4')\n",
      "gene_px                    (no streamer)              asdtype('>f4')\n",
      "gene_py                    (no streamer)              asdtype('>f4')\n",
      "gene_pz                    (no streamer)              asdtype('>f4')\n",
      "Empz                       (no streamer)              asdtype('>f4')\n",
      "pth                        (no streamer)              asdtype('>f4')\n",
      "x_sigma                    (no streamer)              asdtype('>f4')\n",
      "x_e                        (no streamer)              asdtype('>f4')\n",
      "x_da                       (no streamer)              asdtype('>f4')\n",
      "y_h                        (no streamer)              asdtype('>f4')\n",
      "Q2_h                       (no streamer)              asdtype('>f4')\n",
      "x_h                        (no streamer)              asdtype('>f4')\n",
      "gen_s                      (no streamer)              asdtype('>f4')\n",
      "gen_x                      (no streamer)              asdtype('>f4')\n",
      "HFS_E                      (no streamer)              asdtype('>f4')\n",
      "gen_e_e                    (no streamer)              asdtype('>f4')\n",
      "gen_e_pt                   (no streamer)              asdtype('>f4')\n",
      "gen_e_eta                  (no streamer)              asdtype('>f4')\n",
      "rec_e_eta                  (no streamer)              asdtype('>f4')\n",
      "gen_e_phi                  (no streamer)              asdtype('>f4')\n",
      "gen_e_theta                (no streamer)              asdtype('>f4')\n",
      "HFS_pt                     (no streamer)              asdtype('>f4')\n",
      "HFS_eta                    (no streamer)              asdtype('>f4')\n",
      "HFS_phi                    (no streamer)              asdtype('>f4')\n",
      "HFS_theta                  (no streamer)              asdtype('>f4')\n",
      "HFS_gamma                  (no streamer)              asdtype('>f4')\n",
      "gen_HFS_px                 (no streamer)              asdtype('>f4')\n",
      "gen_HFS_py                 (no streamer)              asdtype('>f4')\n",
      "gen_HFS_pz                 (no streamer)              asdtype('>f4')\n",
      "gen_HFS_e                  (no streamer)              asdtype('>f4')\n",
      "gen_HFS_pt                 (no streamer)              asdtype('>f4')\n",
      "gen_HFS_eta                (no streamer)              asdtype('>f4')\n",
      "gen_HFS_phi                (no streamer)              asdtype('>f4')\n",
      "gen_HFS_theta              (no streamer)              asdtype('>f4')\n",
      "beam_electron_energy       (no streamer)              asdtype('>f4')\n",
      "beam_proton_energy         (no streamer)              asdtype('>f4')\n"
     ]
    }
   ],
   "source": [
    "input_file = 'mini-tree-gen-HFS-etamax9.0.root'\n",
    "\n",
    "ur_file = uproot3.open(input_file)\n",
    "\n",
    "print (ur_file.keys()) \n",
    "ur_tree = ur_file['minitree']\n",
    "print(ur_tree)\n",
    "ur_tree.show()\n",
    "\n",
    "pandas_df   =  ur_tree.pandas.df(['*'], entrystop=3e7,flatten=True)\n",
    "\n",
    "pandas_df.eval( 'gene_e = sqrt( gene_px*gene_px + gene_py*gene_py + gene_pz*gene_pz)', inplace=True )\n",
    "\n",
    "pandas_df.eval( 'e_e = sqrt( e_px*e_px + e_py*e_py + e_pz*e_pz)', inplace=True )\n",
    "\n",
    "pandas_df.eval( 'e_pt = sqrt( e_px*e_px + e_py*e_py)', inplace=True )\n",
    "\n",
    "pandas_df.eval( 'e_phi = arctan2( e_py, e_px )', inplace=True )\n",
    "\n",
    "pandas_df.eval( 'gene_pt = gen_e_pt', inplace=True )\n",
    "pandas_df.eval( 'gene_phi = gen_e_phi', inplace=True )\n",
    "\n",
    "pandas_df.eval( 'dphi = e_phi - HFS_phi', inplace=True )\n",
    "pandas_df.eval( 'dphi = (abs(dphi)<3.14159265)*(dphi)+(dphi>3.14159265)*(dphi-2*3.14159265) + (dphi<-3.14159265)*(dphi+2*3.14159265)', inplace=True )\n",
    "pandas_df.eval( 'dphi = (dphi>0)*dphi + (dphi<0)*(dphi+2*3.14159265)', inplace=True )\n",
    "\n",
    "\n",
    "pandas_df.eval( 'gen_dphi = 3.141592653589793', inplace=True )\n",
    "\n",
    "pandas_df.eval( 'HFS_Empz = HFS_E - HFS_pz', inplace=True )\n",
    "pandas_df.eval( 'e_Empz = e_e - e_pz', inplace=True )\n",
    "\n",
    "pandas_df.eval( 'gen_HFS_Empz = gen_HFS_e - gen_HFS_pz', inplace=True )\n",
    "pandas_df.eval( 'gene_Empz = gene_e - gene_pz', inplace=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae19f05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q2_e</th>\n",
       "      <th>Q2_sigma</th>\n",
       "      <th>Q2_esigma</th>\n",
       "      <th>Q2_da</th>\n",
       "      <th>gen_Q2</th>\n",
       "      <th>y_e</th>\n",
       "      <th>y_sigma</th>\n",
       "      <th>y_esigma</th>\n",
       "      <th>y_da</th>\n",
       "      <th>HFS_px</th>\n",
       "      <th>...</th>\n",
       "      <th>e_pt</th>\n",
       "      <th>e_phi</th>\n",
       "      <th>gene_pt</th>\n",
       "      <th>gene_phi</th>\n",
       "      <th>dphi</th>\n",
       "      <th>gen_dphi</th>\n",
       "      <th>HFS_Empz</th>\n",
       "      <th>e_Empz</th>\n",
       "      <th>gen_HFS_Empz</th>\n",
       "      <th>gene_Empz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468.797363</td>\n",
       "      <td>446.056671</td>\n",
       "      <td>468.797363</td>\n",
       "      <td>493.576904</td>\n",
       "      <td>494.087463</td>\n",
       "      <td>0.068597</td>\n",
       "      <td>0.021113</td>\n",
       "      <td>0.022189</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>21.946800</td>\n",
       "      <td>...</td>\n",
       "      <td>20.895912</td>\n",
       "      <td>-2.841402</td>\n",
       "      <td>22.023188</td>\n",
       "      <td>-2.841402</td>\n",
       "      <td>3.163943</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>1.108887</td>\n",
       "      <td>51.413383</td>\n",
       "      <td>1.013672</td>\n",
       "      <td>54.186996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1894.759888</td>\n",
       "      <td>1839.243164</td>\n",
       "      <td>1894.759888</td>\n",
       "      <td>1972.203491</td>\n",
       "      <td>1961.159668</td>\n",
       "      <td>0.297868</td>\n",
       "      <td>0.276675</td>\n",
       "      <td>0.285026</td>\n",
       "      <td>0.269170</td>\n",
       "      <td>-8.184976</td>\n",
       "      <td>...</td>\n",
       "      <td>36.474258</td>\n",
       "      <td>-1.340417</td>\n",
       "      <td>37.752453</td>\n",
       "      <td>-1.340417</td>\n",
       "      <td>3.154179</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>14.824951</td>\n",
       "      <td>38.757668</td>\n",
       "      <td>15.084839</td>\n",
       "      <td>40.115883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276.638916</td>\n",
       "      <td>275.250427</td>\n",
       "      <td>276.638916</td>\n",
       "      <td>277.117676</td>\n",
       "      <td>275.729858</td>\n",
       "      <td>0.292298</td>\n",
       "      <td>0.288728</td>\n",
       "      <td>0.290185</td>\n",
       "      <td>0.291074</td>\n",
       "      <td>9.406549</td>\n",
       "      <td>...</td>\n",
       "      <td>13.992063</td>\n",
       "      <td>2.275361</td>\n",
       "      <td>13.946084</td>\n",
       "      <td>2.275361</td>\n",
       "      <td>3.098422</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>15.857819</td>\n",
       "      <td>39.065125</td>\n",
       "      <td>16.263672</td>\n",
       "      <td>38.936760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>238.202896</td>\n",
       "      <td>238.123535</td>\n",
       "      <td>238.202896</td>\n",
       "      <td>238.850800</td>\n",
       "      <td>247.360428</td>\n",
       "      <td>0.315208</td>\n",
       "      <td>0.314980</td>\n",
       "      <td>0.315085</td>\n",
       "      <td>0.313346</td>\n",
       "      <td>-12.571018</td>\n",
       "      <td>...</td>\n",
       "      <td>12.771819</td>\n",
       "      <td>-0.306055</td>\n",
       "      <td>13.262822</td>\n",
       "      <td>-0.306055</td>\n",
       "      <td>3.051179</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>17.381104</td>\n",
       "      <td>37.800510</td>\n",
       "      <td>15.946594</td>\n",
       "      <td>39.253719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>338.968628</td>\n",
       "      <td>354.665100</td>\n",
       "      <td>338.968628</td>\n",
       "      <td>315.247253</td>\n",
       "      <td>321.091949</td>\n",
       "      <td>0.580202</td>\n",
       "      <td>0.598781</td>\n",
       "      <td>0.572280</td>\n",
       "      <td>0.609580</td>\n",
       "      <td>-10.682003</td>\n",
       "      <td>...</td>\n",
       "      <td>11.928894</td>\n",
       "      <td>-0.382090</td>\n",
       "      <td>11.299782</td>\n",
       "      <td>-0.382090</td>\n",
       "      <td>3.116822</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>34.583248</td>\n",
       "      <td>23.172871</td>\n",
       "      <td>33.249878</td>\n",
       "      <td>21.950768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82850</th>\n",
       "      <td>670.273315</td>\n",
       "      <td>696.540771</td>\n",
       "      <td>670.273315</td>\n",
       "      <td>645.013794</td>\n",
       "      <td>644.488586</td>\n",
       "      <td>-0.009858</td>\n",
       "      <td>0.028225</td>\n",
       "      <td>0.027160</td>\n",
       "      <td>0.028199</td>\n",
       "      <td>-11.539625</td>\n",
       "      <td>...</td>\n",
       "      <td>26.016937</td>\n",
       "      <td>-1.063816</td>\n",
       "      <td>25.016092</td>\n",
       "      <td>-1.063816</td>\n",
       "      <td>3.189495</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>1.619064</td>\n",
       "      <td>55.744118</td>\n",
       "      <td>1.600708</td>\n",
       "      <td>53.599701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82851</th>\n",
       "      <td>319.102203</td>\n",
       "      <td>311.246521</td>\n",
       "      <td>319.102203</td>\n",
       "      <td>319.882477</td>\n",
       "      <td>318.792084</td>\n",
       "      <td>0.366897</td>\n",
       "      <td>0.350918</td>\n",
       "      <td>0.359775</td>\n",
       "      <td>0.365349</td>\n",
       "      <td>8.577925</td>\n",
       "      <td>...</td>\n",
       "      <td>14.213531</td>\n",
       "      <td>-2.177933</td>\n",
       "      <td>14.199718</td>\n",
       "      <td>-2.177933</td>\n",
       "      <td>3.232355</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>18.893814</td>\n",
       "      <td>34.947262</td>\n",
       "      <td>20.287170</td>\n",
       "      <td>34.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82852</th>\n",
       "      <td>1181.045532</td>\n",
       "      <td>1121.059570</td>\n",
       "      <td>1181.045532</td>\n",
       "      <td>1201.989502</td>\n",
       "      <td>1243.584961</td>\n",
       "      <td>0.401450</td>\n",
       "      <td>0.369423</td>\n",
       "      <td>0.389190</td>\n",
       "      <td>0.390836</td>\n",
       "      <td>20.893599</td>\n",
       "      <td>...</td>\n",
       "      <td>26.587864</td>\n",
       "      <td>-2.651498</td>\n",
       "      <td>27.995762</td>\n",
       "      <td>-2.651498</td>\n",
       "      <td>3.097349</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>19.356415</td>\n",
       "      <td>33.039944</td>\n",
       "      <td>20.410950</td>\n",
       "      <td>34.789494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82853</th>\n",
       "      <td>545.924133</td>\n",
       "      <td>558.406006</td>\n",
       "      <td>545.924133</td>\n",
       "      <td>533.195801</td>\n",
       "      <td>533.533325</td>\n",
       "      <td>-0.001577</td>\n",
       "      <td>0.020811</td>\n",
       "      <td>0.020346</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>22.185625</td>\n",
       "      <td>...</td>\n",
       "      <td>23.383434</td>\n",
       "      <td>-3.049639</td>\n",
       "      <td>22.852703</td>\n",
       "      <td>-3.049639</td>\n",
       "      <td>3.121238</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>1.175034</td>\n",
       "      <td>55.286987</td>\n",
       "      <td>1.168396</td>\n",
       "      <td>54.032143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82854</th>\n",
       "      <td>234.940277</td>\n",
       "      <td>237.397141</td>\n",
       "      <td>234.940277</td>\n",
       "      <td>226.328796</td>\n",
       "      <td>232.769821</td>\n",
       "      <td>0.115104</td>\n",
       "      <td>0.124262</td>\n",
       "      <td>0.122976</td>\n",
       "      <td>0.147539</td>\n",
       "      <td>-8.611775</td>\n",
       "      <td>...</td>\n",
       "      <td>14.418653</td>\n",
       "      <td>-0.885983</td>\n",
       "      <td>14.285447</td>\n",
       "      <td>-0.885983</td>\n",
       "      <td>3.010261</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>6.931015</td>\n",
       "      <td>48.846230</td>\n",
       "      <td>6.805542</td>\n",
       "      <td>48.394966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82855 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q2_e     Q2_sigma    Q2_esigma        Q2_da       gen_Q2  \\\n",
       "entry                                                                    \n",
       "0       468.797363   446.056671   468.797363   493.576904   494.087463   \n",
       "1      1894.759888  1839.243164  1894.759888  1972.203491  1961.159668   \n",
       "2       276.638916   275.250427   276.638916   277.117676   275.729858   \n",
       "3       238.202896   238.123535   238.202896   238.850800   247.360428   \n",
       "4       338.968628   354.665100   338.968628   315.247253   321.091949   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "82850   670.273315   696.540771   670.273315   645.013794   644.488586   \n",
       "82851   319.102203   311.246521   319.102203   319.882477   318.792084   \n",
       "82852  1181.045532  1121.059570  1181.045532  1201.989502  1243.584961   \n",
       "82853   545.924133   558.406006   545.924133   533.195801   533.533325   \n",
       "82854   234.940277   237.397141   234.940277   226.328796   232.769821   \n",
       "\n",
       "            y_e   y_sigma  y_esigma      y_da     HFS_px  ...       e_pt  \\\n",
       "entry                                                     ...              \n",
       "0      0.068597  0.021113  0.022189  0.019365  21.946800  ...  20.895912   \n",
       "1      0.297868  0.276675  0.285026  0.269170  -8.184976  ...  36.474258   \n",
       "2      0.292298  0.288728  0.290185  0.291074   9.406549  ...  13.992063   \n",
       "3      0.315208  0.314980  0.315085  0.313346 -12.571018  ...  12.771819   \n",
       "4      0.580202  0.598781  0.572280  0.609580 -10.682003  ...  11.928894   \n",
       "...         ...       ...       ...       ...        ...  ...        ...   \n",
       "82850 -0.009858  0.028225  0.027160  0.028199 -11.539625  ...  26.016937   \n",
       "82851  0.366897  0.350918  0.359775  0.365349   8.577925  ...  14.213531   \n",
       "82852  0.401450  0.369423  0.389190  0.390836  20.893599  ...  26.587864   \n",
       "82853 -0.001577  0.020811  0.020346  0.021775  22.185625  ...  23.383434   \n",
       "82854  0.115104  0.124262  0.122976  0.147539  -8.611775  ...  14.418653   \n",
       "\n",
       "          e_phi    gene_pt  gene_phi      dphi  gen_dphi   HFS_Empz  \\\n",
       "entry                                                                 \n",
       "0     -2.841402  22.023188 -2.841402  3.163943  3.141593   1.108887   \n",
       "1     -1.340417  37.752453 -1.340417  3.154179  3.141593  14.824951   \n",
       "2      2.275361  13.946084  2.275361  3.098422  3.141593  15.857819   \n",
       "3     -0.306055  13.262822 -0.306055  3.051179  3.141593  17.381104   \n",
       "4     -0.382090  11.299782 -0.382090  3.116822  3.141593  34.583248   \n",
       "...         ...        ...       ...       ...       ...        ...   \n",
       "82850 -1.063816  25.016092 -1.063816  3.189495  3.141593   1.619064   \n",
       "82851 -2.177933  14.199718 -2.177933  3.232355  3.141593  18.893814   \n",
       "82852 -2.651498  27.995762 -2.651498  3.097349  3.141593  19.356415   \n",
       "82853 -3.049639  22.852703 -3.049639  3.121238  3.141593   1.175034   \n",
       "82854 -0.885983  14.285447 -0.885983  3.010261  3.141593   6.931015   \n",
       "\n",
       "          e_Empz  gen_HFS_Empz  gene_Empz  \n",
       "entry                                      \n",
       "0      51.413383      1.013672  54.186996  \n",
       "1      38.757668     15.084839  40.115883  \n",
       "2      39.065125     16.263672  38.936760  \n",
       "3      37.800510     15.946594  39.253719  \n",
       "4      23.172871     33.249878  21.950768  \n",
       "...          ...           ...        ...  \n",
       "82850  55.744118      1.600708  53.599701  \n",
       "82851  34.947262     20.287170  34.913300  \n",
       "82852  33.039944     20.410950  34.789494  \n",
       "82853  55.286987      1.168396  54.032143  \n",
       "82854  48.846230      6.805542  48.394966  \n",
       "\n",
       "[82855 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e3652",
   "metadata": {},
   "source": [
    "## Set up the machine learning stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4327ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "#physical_devices = tf.config.list_physical_devices('GPU') \n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('CPU') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c15ddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Shape of X\n",
      "(82855, 9)\n",
      "\n",
      "\n",
      " Shape of Y\n",
      "(82855, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.c_[ pandas_df['gen_HFS_pz'].to_numpy(),\n",
    "          pandas_df['gen_HFS_e'].to_numpy(),\n",
    "          pandas_df['gen_HFS_pt'].to_numpy(),\n",
    "          pandas_df['gen_HFS_Empz'].to_numpy(),\n",
    "          pandas_df['gene_pz'].to_numpy(),\n",
    "          pandas_df['gene_e'].to_numpy(),\n",
    "          pandas_df['gene_pt'].to_numpy(),\n",
    "          pandas_df['gene_Empz'].to_numpy(),\n",
    "          pandas_df['gen_dphi'].to_numpy(),\n",
    "         ]\n",
    "\n",
    "Y = pandas_df['gen_x'].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "Y = Y.reshape(-1,1)\n",
    "scalerY = StandardScaler()\n",
    "scalerY.fit(Y)\n",
    "Y = scalerY.transform(Y)\n",
    "\n",
    "GY = pandas_df['gen_y'].to_numpy()\n",
    "\n",
    "print('\\n\\n Shape of X')\n",
    "print( X.shape )\n",
    "\n",
    "print('\\n\\n Shape of Y')\n",
    "print( Y.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab47ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, GY_train, GY_test = train_test_split( X, Y, GY, test_size=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6574201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=9, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='selu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "myloss = tf.keras.losses.Huber()\n",
    "\n",
    "model.compile(loss=myloss, optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df831e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "83/83 [==============================] - 1s 5ms/step - loss: 0.3497 - accuracy: 0.0000e+00 - val_loss: 0.1781 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.0000e+00 - val_loss: 0.1462 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1899 - accuracy: 0.0000e+00 - val_loss: 0.1364 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.0000e+00 - val_loss: 0.1299 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.0000e+00 - val_loss: 0.1226 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1512 - accuracy: 0.0000e+00 - val_loss: 0.1159 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.0000e+00 - val_loss: 0.1087 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.0000e+00 - val_loss: 0.1030 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.0000e+00 - val_loss: 0.0966 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.0000e+00 - val_loss: 0.0899 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.0000e+00 - val_loss: 0.0838 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.0000e+00 - val_loss: 0.0786 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.0000e+00 - val_loss: 0.0735 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0981 - accuracy: 0.0000e+00 - val_loss: 0.0686 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0920 - accuracy: 0.0000e+00 - val_loss: 0.0644 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.0000e+00 - val_loss: 0.0601 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.0000e+00 - val_loss: 0.0562 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0796 - accuracy: 0.0000e+00 - val_loss: 0.0529 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.0000e+00 - val_loss: 0.0495 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.0000e+00 - val_loss: 0.0466 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0682 - accuracy: 0.0000e+00 - val_loss: 0.0437 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.0000e+00 - val_loss: 0.0411 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0600 - accuracy: 0.0000e+00 - val_loss: 0.0368 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.0000e+00 - val_loss: 0.0348 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.0000e+00 - val_loss: 0.0330 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.0000e+00 - val_loss: 0.0313 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.0000e+00 - val_loss: 0.0298 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.0000e+00 - val_loss: 0.0281 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.0000e+00 - val_loss: 0.0275 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.0000e+00 - val_loss: 0.0258 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.0000e+00 - val_loss: 0.0257 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0240 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0229 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 0.0000e+00 - val_loss: 0.0222 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 0.0208 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.0000e+00 - val_loss: 0.0198 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.0000e+00 - val_loss: 0.0195 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.0000e+00 - val_loss: 0.0183 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.0000e+00 - val_loss: 0.0169 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.0000e+00 - val_loss: 0.0145 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 0.0000e+00 - val_loss: 0.0139 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.0000e+00 - val_loss: 0.0139 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.0000e+00 - val_loss: 0.0129 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.0000e+00 - val_loss: 0.0128 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0000e+00 - val_loss: 0.0125 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.0000e+00 - val_loss: 0.0115 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - val_loss: 0.0111 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - val_loss: 0.0108 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - val_loss: 0.0104 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - val_loss: 0.0103 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - val_loss: 0.0095 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - val_loss: 0.0095 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - val_loss: 0.0094 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0085 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - val_loss: 0.0081 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - val_loss: 0.0082 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - val_loss: 0.0076 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - val_loss: 0.0075 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - val_loss: 0.0076 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - val_loss: 0.0070 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - val_loss: 0.0067 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - val_loss: 0.0067 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - val_loss: 0.0062 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0000e+00 - val_loss: 0.0059 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - val_loss: 0.0058 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0058 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.0000e+00 - val_loss: 0.0057 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.0000e+00 - val_loss: 0.0053 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.0000e+00 - val_loss: 0.0054 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.0000e+00 - val_loss: 0.0049 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.0000e+00 - val_loss: 0.0049 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.0000e+00 - val_loss: 0.0048 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.0000e+00 - val_loss: 0.0045 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.0000e+00 - val_loss: 0.0049 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.0000e+00 - val_loss: 0.0032 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.0000e+00 - val_loss: 0.0031 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.0000e+00 - val_loss: 0.0033 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.0000e+00 - val_loss: 0.0030 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.0000e+00 - val_loss: 0.0030 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.0000e+00 - val_loss: 0.0028 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.0000e+00 - val_loss: 0.0028 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.0000e+00 - val_loss: 0.0028 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0104 - accuracy: 0.0000e+00 - val_loss: 0.0024 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.0000e+00 - val_loss: 0.0023 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.0000e+00 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.0000e+00 - val_loss: 0.0023 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.0000e+00 - val_loss: 0.0023 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 0.0000e+00 - val_loss: 0.0025 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 0.0000e+00 - val_loss: 0.0020 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 0.0000e+00 - val_loss: 0.0020 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 0.0000e+00 - val_loss: 0.0021 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.0000e+00 - val_loss: 0.0023 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - val_loss: 0.0020 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.0000e+00 - val_loss: 0.0020 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.0000e+00 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0065 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/300\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0065 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0065 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 0.0000e+00 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 9.9042e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.0000e+00 - val_loss: 9.6058e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/300\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 9.7802e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 9.4436e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 9.9649e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 7.9856e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 9.0726e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 8.3910e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 9.1480e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 9.1383e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 8.4178e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/300\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 9.8334e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 9.6582e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 0.0000e+00 - val_loss: 8.3011e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.0000e+00 - val_loss: 8.9348e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 9.1982e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 9.3806e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 8.4604e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/300\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/300\n",
      "75/83 [==========================>...] - ETA: 0s - loss: 0.0044 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, epochs=300, batch_size=500, verbose=1, validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17973b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypreds = model.predict(X_test,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_=plt.hist(mypreds[:,0]/Y_test[:,0],bins=np.linspace(0,3,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mypreds[:,0],Y_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(15,15))\n",
    "ax.hist2d(mypreds[:,0],Y_test[:,0],bins=200, norm=mpl.colors.LogNorm())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(24,6),sharey='row')\n",
    "\n",
    "ax[0].hist(mypreds[:,0][(GY_test > 0.5)*(GY_test < 0.8)]/Y_test[(GY_test > 0.5)*(GY_test < 0.8)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[1].hist(mypreds[:,0][(GY_test > 0.2)*(GY_test < 0.5)]/Y_test[(GY_test > 0.2)*(GY_test < 0.5)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[2].hist(mypreds[:,0][(GY_test > 0.1)*(GY_test < 0.2)]/Y_test[(GY_test > 0.1)*(GY_test < 0.2)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[3].hist(mypreds[:,0][(GY_test > 0.05)*(GY_test < 0.1)]/Y_test[(GY_test > 0.05)*(GY_test < 0.1)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[4].hist(mypreds[:,0][(GY_test > 0.01)*(GY_test < 0.05)]/Y_test[(GY_test > 0.01)*(GY_test < 0.05)][:,0],density=True,bins=100,range=(0,2))\n",
    "\n",
    "ax[0].set_title('$0.5<y_{true}<0.8$')\n",
    "ax[1].set_title('$0.2<y_{true}<0.5$')\n",
    "ax[2].set_title('$0.1<y_{true}<0.2$')\n",
    "ax[3].set_title('$0.05<y_{true}<0.1$')\n",
    "ax[4].set_title('$0.01<y_{true}<0.05$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba7ffb",
   "metadata": {},
   "source": [
    "## Some conclusions, gen HFS, gen e\n",
    "\n",
    "If I feed it the true HFS and scattered electron, the NN can learn how to compute X, which is good, but not surprising.  The longer it trains, the better it gets.  Note that it could figure it all out just from the scattered electron.  Try gen HFS with reco electron.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[ pandas_df['gen_HFS_pz'].to_numpy(),\n",
    "          pandas_df['gen_HFS_e'].to_numpy(),\n",
    "          pandas_df['gen_HFS_pt'].to_numpy(),\n",
    "          pandas_df['gen_HFS_Empz'].to_numpy(),\n",
    "          pandas_df['e_pz'].to_numpy(),\n",
    "          pandas_df['e_e'].to_numpy(),\n",
    "          pandas_df['e_pt'].to_numpy(),\n",
    "          pandas_df['e_Empz'].to_numpy(),\n",
    "          pandas_df['dphi'].to_numpy(),\n",
    "         ]\n",
    "\n",
    "Y = pandas_df['gen_x'].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "Y = Y.reshape(-1,1)\n",
    "scalerY = StandardScaler()\n",
    "scalerY.fit(Y)\n",
    "Y = scalerY.transform(Y)\n",
    "\n",
    "GY = pandas_df['gen_y'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce562a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, GY_train, GY_test = train_test_split( X, Y, GY, test_size=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=9, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='selu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "myloss = tf.keras.losses.Huber()\n",
    "\n",
    "model.compile(loss=myloss, optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51af79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, Y_train, epochs=300, batch_size=500, verbose=1, validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypreds = model.predict(X_test,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_=plt.hist(mypreds[:,0]/Y_test[:,0],bins=np.linspace(0,3,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mypreds[:,0],Y_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53148557",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(15,15))\n",
    "ax.hist2d(mypreds[:,0],Y_test[:,0],bins=200, norm=mpl.colors.LogNorm())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(24,6),sharey='row')\n",
    "\n",
    "ax[0].hist(mypreds[:,0][(GY_test > 0.5)*(GY_test < 0.8)]/Y_test[(GY_test > 0.5)*(GY_test < 0.8)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[1].hist(mypreds[:,0][(GY_test > 0.2)*(GY_test < 0.5)]/Y_test[(GY_test > 0.2)*(GY_test < 0.5)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[2].hist(mypreds[:,0][(GY_test > 0.1)*(GY_test < 0.2)]/Y_test[(GY_test > 0.1)*(GY_test < 0.2)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[3].hist(mypreds[:,0][(GY_test > 0.05)*(GY_test < 0.1)]/Y_test[(GY_test > 0.05)*(GY_test < 0.1)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[4].hist(mypreds[:,0][(GY_test > 0.01)*(GY_test < 0.05)]/Y_test[(GY_test > 0.01)*(GY_test < 0.05)][:,0],density=True,bins=100,range=(0,2))\n",
    "\n",
    "ax[0].set_title('$0.5<y_{true}<0.8$')\n",
    "ax[1].set_title('$0.2<y_{true}<0.5$')\n",
    "ax[2].set_title('$0.1<y_{true}<0.2$')\n",
    "ax[3].set_title('$0.05<y_{true}<0.1$')\n",
    "ax[4].set_title('$0.01<y_{true}<0.05$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a93335",
   "metadata": {},
   "source": [
    "## Conclusions with gen HFS, reco e\n",
    "\n",
    "Also works pretty well.  Now try with reco everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f442c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[ pandas_df['HFS_pz'].to_numpy(),\n",
    "          pandas_df['HFS_E'].to_numpy(),\n",
    "          pandas_df['HFS_pt'].to_numpy(),\n",
    "          pandas_df['HFS_Empz'].to_numpy(),\n",
    "          pandas_df['e_pz'].to_numpy(),\n",
    "          pandas_df['e_e'].to_numpy(),\n",
    "          pandas_df['e_pt'].to_numpy(),\n",
    "          pandas_df['e_Empz'].to_numpy(),\n",
    "          pandas_df['dphi'].to_numpy(),\n",
    "         ]\n",
    "\n",
    "Y = pandas_df['gen_x'].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "Y = Y.reshape(-1,1)\n",
    "scalerY = StandardScaler()\n",
    "scalerY.fit(Y)\n",
    "Y = scalerY.transform(Y)\n",
    "\n",
    "GY = pandas_df['gen_y'].to_numpy()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, GY_train, GY_test = train_test_split( X, Y, GY, test_size=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=9, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='selu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "myloss = tf.keras.losses.Huber()\n",
    "\n",
    "model.compile(loss=myloss, optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfa563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, Y_train, epochs=300, batch_size=500, verbose=1, validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904eb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypreds = model.predict(X_test,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ae29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_=plt.hist(mypreds[:,0]/Y_test[:,0],bins=np.linspace(0,3,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c447b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mypreds[:,0],Y_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adea995",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(15,15))\n",
    "ax.hist2d(mypreds[:,0],Y_test[:,0],bins=200, norm=mpl.colors.LogNorm())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(24,6),sharey='row')\n",
    "\n",
    "ax[0].hist(mypreds[:,0][(GY_test > 0.5)*(GY_test < 0.8)]/Y_test[(GY_test > 0.5)*(GY_test < 0.8)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[1].hist(mypreds[:,0][(GY_test > 0.2)*(GY_test < 0.5)]/Y_test[(GY_test > 0.2)*(GY_test < 0.5)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[2].hist(mypreds[:,0][(GY_test > 0.1)*(GY_test < 0.2)]/Y_test[(GY_test > 0.1)*(GY_test < 0.2)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[3].hist(mypreds[:,0][(GY_test > 0.05)*(GY_test < 0.1)]/Y_test[(GY_test > 0.05)*(GY_test < 0.1)][:,0],density=True,bins=100,range=(0,2))\n",
    "ax[4].hist(mypreds[:,0][(GY_test > 0.01)*(GY_test < 0.05)]/Y_test[(GY_test > 0.01)*(GY_test < 0.05)][:,0],density=True,bins=100,range=(0,2))\n",
    "\n",
    "ax[0].set_title('$0.5<y_{true}<0.8$')\n",
    "ax[1].set_title('$0.2<y_{true}<0.5$')\n",
    "ax[2].set_title('$0.1<y_{true}<0.2$')\n",
    "ax[3].set_title('$0.05<y_{true}<0.1$')\n",
    "ax[4].set_title('$0.01<y_{true}<0.05$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c0a5d",
   "metadata": {},
   "source": [
    "## Some conclusions on reco HFS, reco e\n",
    "\n",
    "Not too bad, but not perfect.  Hit a plateau in training after several chunks of 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab48415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf287b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
